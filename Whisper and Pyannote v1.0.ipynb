{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a84fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tools\n",
    "# speechbrain (used for speaker embedding)\n",
    "!pip install -qq torch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 torchtext==0.12.0\n",
    "!pip install -qq speechbrain==0.5.12\n",
    "\n",
    "# pyannote.audio (used for speaker diarization)\n",
    "!pip install -qq pyannote.audio==2.1.1\n",
    "\n",
    "# OpenAI whisper (used for automatic speech recognition)\n",
    "!pip install -qq git+https://github.com/openai/whisper.git \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d57838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tools\n",
    "import subprocess #if stripping audio from video file\n",
    "import datetime #to print time of start and end of analysis\n",
    "import os\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "from pyannote.audio import Audio\n",
    "\n",
    "import whisper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21282897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c97170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables\n",
    "audio = Audio(sample_rate=16000, mono=True)\n",
    "model_size = \"large-v2\"\n",
    "base_model = whisper.load_model(model_size)\n",
    "\n",
    "speakers = {'SPEAKER_00':('Talare_1', 'white', 'darkorange'), \n",
    "            'SPEAKER_01':('Talare_2', '#e1ffc7', 'darkgreen'), \n",
    "            'SPEAKER_02':('Talare_3', '#e1ffc7', 'darkgreen'), \n",
    "            'SPEAKER_03':('Talare_4', '#e1ffc7', 'darkgreen'), \n",
    "            'SPEAKER_04':('Talare_5', '#e1ffc7', 'darkgreen'), \n",
    "            'SPEAKER_05':('Talare_6', '#e1ffc7', 'darkgreen'), \n",
    "            'SPEAKER_06':('Talare_7', '#e1ffc7', 'darkgreen'), \n",
    "            'SPEAKER_07':('Talare_8', '#e1ffc7', 'darkgreen'), \n",
    "            'SPEAKER_08':('Talare_9', '#e1ffc7', 'darkgreen'), \n",
    "            'SPEAKER_09':('Talare_10', '#e1ffc7', 'darkgreen'), \n",
    "            'SPEAKER_10':('Talare_11', '#e1ffc7', 'darkgreen') }\n",
    "def_boxclr = 'white'\n",
    "def_spkrclr = 'orange'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3647fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for isolating audio\n",
    "\n",
    "def isolate_audio(file):\n",
    "    subprocess.call(['ffmpeg', '-i', file, 'input_audio.wav', '-y'])\n",
    "    return 'input_audio.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257d8f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def just_text(audio_file, file_path):\n",
    "    result = base_model.transcribe(audio_file)\n",
    "    with open(os.path.splitext(file_path)[0] + '_text.txt', 'w') as file:\n",
    "        s = \"\".join(result[\"text\"])\n",
    "        file.write(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for speaker diarization\n",
    "def speaker_dz(audio_file):\n",
    "    start = datetime.datetime.now()\n",
    "    pipeline = Pipeline.from_pretrained('pyannote/speaker-diarization@2.1', \n",
    "                                    use_auth_token=True)\n",
    "    \n",
    "    who_speaks_when = pipeline(audio_file, \n",
    "                                      num_speakers=None,  # these values can be\n",
    "                                      min_speakers=None,  # provided by the user\n",
    "                                      max_speakers=None)  # when they are known\n",
    "    who_speaks_when\n",
    "    with open(\"diarization.txt\", \"w\") as text_file:\n",
    "        text_file.write(str(who_speaks_when))\n",
    "    end = datetime.datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cceb04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def millisec(timeStr):\n",
    "  spl = timeStr.split(\":\")\n",
    "  s = (int)((int(spl[0]) * 60 * 60 + int(spl[1]) * 60 + float(spl[2]) )* 1000)\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c737b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing audio files according to dz\n",
    "import re\n",
    "\n",
    "def transcribe(audio_file):\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    dzs = open('diarization.txt').read().splitlines()\n",
    "    groups = []\n",
    "    g = []\n",
    "    lastend = 0\n",
    "    \n",
    "    for d in dzs:   \n",
    "        if g and (g[0].split()[-1] != d.split()[-1]):      #same speaker\n",
    "            groups.append(g)\n",
    "            g = []\n",
    "            \n",
    "        g.append(d)\n",
    "        end = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=d)[1]\n",
    "        end = millisec(end)\n",
    "        \n",
    "        if (lastend > end):       #segment engulfed by a previous segment\n",
    "            groups.append(g)\n",
    "            g = [] \n",
    "        else:\n",
    "            lastend = end\n",
    "    if g:\n",
    "      groups.append(g)\n",
    "    print(*groups, sep='\\n')\n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "    gidx = -1\n",
    "    for g in groups:\n",
    "      start = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=g[0])[0]\n",
    "      end = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=g[-1])[1]\n",
    "      start = millisec(start) #- spacermilli\n",
    "      end = millisec(end)  #- spacermilli\n",
    "      print(start, end)\n",
    "      gidx += 1\n",
    "      audio[start:end].export(str(gidx) + '.wav', format='wav')\n",
    "        \n",
    "    for i in range(gidx+1):\n",
    "      !whisper {str(i) + '.wav'} --language sv --model large-v2\n",
    "    end = datetime.datetime.now()\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454a9e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(groups, file_path):\n",
    "    import webvtt\n",
    "    spacermilli = 0\n",
    "    txt = list(\"\")\n",
    "    gidx = -1\n",
    "    for g in groups:\n",
    "        shift = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=g[0])[0]\n",
    "        shift = millisec(shift) - spacermilli #the start time in the original video\n",
    "        shift=max(shift, 0)\n",
    "        gidx += 1\n",
    "        captions = [[(int)(millisec(caption.start)), (int)(millisec(caption.end)),  caption.text] for caption in webvtt.read(str(gidx) + '.wav.vtt')]\n",
    "        #captions = (list) webvtt.read(str(gidx) + '.wav.vtt')\n",
    "\n",
    "        if captions:\n",
    "            speaker = g[0].split()[-1]\n",
    "            boxclr = def_boxclr\n",
    "            spkrclr = def_spkrclr\n",
    "            if speaker in speakers:\n",
    "                speaker, boxclr, spkrclr = speakers[speaker]\n",
    "\n",
    "        for c in captions:\n",
    "            start = shift + c[0]\n",
    "            start = start / 1000.0   #time resolution ot youtube is Second.\n",
    "            startStr = '{0:02d}:{1:02d}:{2:06.3f}'.format((int)(start // 3600), (int)(start % 3600 // 60), start % 60)\n",
    "            end = shift + c[1]\n",
    "            end = end / 1000.0   #time resolution ot youtube is Second.\n",
    "            endStr = '{0:02d}:{1:02d}:{2:06.3f}'.format((int)(end // 3600), (int)(end % 3600 // 60), end % 60)\n",
    "\n",
    "        txt.append(f'[{startStr} --> {endStr}] [{speaker}] {c[2]}\\n')\n",
    "\n",
    "    with open(os.path.splitext(file_path)[0] + '.txt', 'w') as file:\n",
    "        s = \"\".join(txt)\n",
    "        file.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00521ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up():\n",
    "    # Get the current working directory\n",
    "    cwd = os.getcwd()\n",
    "    # Iterate over all files and directories in the current working directory\n",
    "    for item in os.listdir(cwd):\n",
    "        # Get the full path of the item\n",
    "        item_path = os.path.join(cwd, item)\n",
    "        # Check if the item is a file and not the script\n",
    "        if os.path.isfile(item_path) and not item_path.endswith('ipynb'):\n",
    "            # Delete the file\n",
    "            os.remove(item_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1695ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that iterates over directory \"Data\"\n",
    "def iterate():\n",
    "    start = datetime.datetime.now()\n",
    "    try:\n",
    "        \n",
    "        for filename in os.listdir(\"Data\"):\n",
    "            start_1 = start = datetime.datetime.now()\n",
    "            file_path = os.path.join(\"Data\", filename)\n",
    "            audio_file = isolate_audio(file_path)\n",
    "            just_text(audio_file, file_path)\n",
    "            speaker_dz(audio_file)\n",
    "            groups = transcribe(audio_file)\n",
    "            write(groups, file_path)\n",
    "            clean_up()\n",
    "            end_1 = start = datetime.datetime.now()\n",
    "            delta_1 = end_1 -  start_1\n",
    "            print(\"Transkription for \" + filename + \"was \" + str(delta_1))\n",
    "            \n",
    "    except Exception as e:\n",
    "            print(f'An error has occured: {e}')\n",
    "        \n",
    "    end = datetime.datetime.now()\n",
    "    delta = end - start\n",
    "    print(\"Runtime for everything was \" + str(delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d1cbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
